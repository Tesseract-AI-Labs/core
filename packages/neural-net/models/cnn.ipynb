{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Select mode\n",
    "mode='train' for the training, mode='predict' for the predictions\n",
    "uncomment the one you want to enable and comment the one you want to enable \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Select mode\n",
    "mode='train' for the training, mode='predict' for the predictions\n",
    "uncomment the one you want to enable and comment the one you want to enable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'train'\n",
    "# mode = 'predict'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone github repo of detr\n",
    "!git clone https://github.com/facebookresearch/detr.git   \n",
    "\n",
    "# general libraries\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "from tqdm.autonotebook import tqdm\n",
    "import re\n",
    "import pydicom\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# torch.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import model_selection\n",
    "\n",
    "# CV\n",
    "import cv2\n",
    "\n",
    "# DETR FUCNTIONS FOR LOSS\n",
    "import sys\n",
    "sys.path.append('./detr/')\n",
    "\n",
    "from detr.models.matcher import HungarianMatcher\n",
    "from detr.models.detr import SetCriterion\n",
    "\n",
    "# albumenatations\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "# Glob\n",
    "from glob import glob\n",
    "\n",
    "# ensembling \n",
    "!pip install ensemble_boxes\n",
    "from tqdm import tqdm\n",
    "from ensemble_boxes import *\n",
    "\n",
    "# mAP\n",
    "!pip install map_boxes \n",
    "from map_boxes import mean_average_precision_for_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thoracic abnormalities\n",
    "CLASSES = [\n",
    "    'Aortic enlargement', 'Atelectasis', 'Calcification', 'Cardiomegaly', 'Consolidation',\n",
    "    'ILD', 'Infiltration', 'Lung Opacity', 'Nodule/Mass', 'Other lesion', \n",
    "    'Pleural effusion', 'Pleural thickening', 'Pneumothorax', 'Pulmonary fibrosis', 'No Finding'\n",
    "]\n",
    "\n",
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing image metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images():\n",
    "    # read the images with size 512x512 \n",
    "    # add the dataset in the data section if it is not added yet\n",
    "    train_df = pd.read_csv('../input/vinbigdata-512-image-dataset/vinbigdata/train.csv')\n",
    "    train_df.fillna(0, inplace=True)\n",
    "    return train_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Scale images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_images(train_df):\n",
    "    # scale the coordinates of the bounding boxes from their initial values to fit the 512x512 images\n",
    "    # set to the images with no object (class 14), bounding box with coordinates [xmin=0 ymin=0 xmax=1 ymax=1]\n",
    "    train_df.loc[train_df[\"class_id\"] == 14, ['x_max', 'y_max']] = 1.0\n",
    "    train_df.loc[train_df[\"class_id\"] == 14, ['x_min', 'y_min']] = 0\n",
    "\n",
    "    # scale the input image coordinates to fit 512x512 image\n",
    "    IMG_SIZE = 512\n",
    "    train_df['xmin'] = (train_df['x_min']/train_df['width'])*IMG_SIZE\n",
    "    train_df['ymin'] = (train_df['y_min']/train_df['height'])*IMG_SIZE\n",
    "    train_df['xmax'] = (train_df['x_max']/train_df['width'])*IMG_SIZE\n",
    "    train_df['ymax'] = (train_df['y_max']/train_df['height'])*IMG_SIZE\n",
    "\n",
    "    # set to the images with no object (class 14), bounding box with coordinates [xmin=0 ymin=0 xmax=1 ymax=1]\n",
    "    train_df.loc[train_df[\"class_id\"] == 14, ['xmax', 'ymax']] = 1.0\n",
    "    train_df.loc[train_df[\"class_id\"] == 14, ['xmin', 'ymin']] = 0\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Define folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_folds(train_df):\n",
    "    unique_images = train_df[\"image_id\"].unique()\n",
    "    df_split = pd.DataFrame(unique_images, columns = ['unique_images']) \n",
    "\n",
    "    # create one column with the number of fold (for the k-fold cross validation)\n",
    "    df_split[\"kfold\"] = -1\n",
    "    df_split = df_split.sample(frac=1).reset_index(drop=True)\n",
    "    y = df_split.unique_images.values\n",
    "    kf = model_selection.GroupKFold(n_splits=5)\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=df_split, y=y, groups=df_split.unique_images.values)):\n",
    "        df_split.loc[v_, \"kfold\"] = f\n",
    "\n",
    "    # annotated boxes from same \"image id\" (image) should be in the same fold [during training each image with its boxes is as one input]\n",
    "    train_df[\"kfold\"] = -1\n",
    "    for ind in train_df.index: \n",
    "         train_df[\"kfold\"][ind] = df_split.loc[ df_split[\"unique_images\"] ==  train_df[\"image_id\"][ind]][\"kfold\"]\n",
    "\n",
    "    train_df.set_index('image_id', inplace=True)\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Weight boxes fusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxes_fusion(df):\n",
    "    # apply weighted boxes fusion for ensemling overlapping annotated boxes\n",
    "    # Default WBF config \n",
    "    iou_thr = 0.75\n",
    "    skip_box_thr = 0.0001\n",
    "    sigma = 0.1\n",
    "    results = []\n",
    "    image_ids = df.index.unique()\n",
    "   \n",
    "    for image_id in tqdm(image_ids, total=len(image_ids)):\n",
    "        # All annotations for the current image.\n",
    "        data = df[df.index == image_id]\n",
    "        kfold = data['kfold'].unique()[0]\n",
    "        data = data.reset_index(drop=True)\n",
    "        \n",
    "        # WBF expects the coordinates in 0-1 range.\n",
    "        max_value = data.iloc[:, 4:].values.max()\n",
    "        data.loc[:, [\"xmin\", \"ymin\", \"xmax\", \"ymax\"]] = data.iloc[:, 4:] / max_value\n",
    "        #print(\"data\",data)\n",
    "        if data.class_id.unique()[0] !=14:\n",
    "            annotations = {}\n",
    "            weights = []\n",
    "            # Loop through all of the annotations\n",
    "            for idx, row in data.iterrows():\n",
    "                rad_id = row[\"rad_id\"]\n",
    "                if rad_id not in annotations:\n",
    "                    annotations[rad_id] = {\n",
    "                        \"boxes_list\": [],\n",
    "                        \"scores_list\": [],\n",
    "                        \"labels_list\": [],\n",
    "                    }\n",
    "                    # We consider all of the radiologists as equal.\n",
    "                    weights.append(1.0)\n",
    "                annotations[rad_id][\"boxes_list\"].append([row[\"xmin\"], row[\"ymin\"], row[\"xmax\"], row[\"ymax\"]])\n",
    "                annotations[rad_id][\"scores_list\"].append(1.0)\n",
    "                annotations[rad_id][\"labels_list\"].append(row[\"class_id\"])\n",
    "\n",
    "            boxes_list = []\n",
    "            scores_list = []\n",
    "            labels_list = []\n",
    "\n",
    "            for annotator in annotations.keys():\n",
    "                boxes_list.append(annotations[annotator][\"boxes_list\"])\n",
    "                scores_list.append(annotations[annotator][\"scores_list\"])\n",
    "                labels_list.append(annotations[annotator][\"labels_list\"])\n",
    "\n",
    "            # Calculate WBF\n",
    "            boxes, scores, labels = weighted_boxes_fusion(boxes_list,\n",
    "                scores_list,\n",
    "                labels_list,\n",
    "                weights=weights,\n",
    "                iou_thr=iou_thr,\n",
    "                skip_box_thr=skip_box_thr\n",
    "            )\n",
    "            for idx, box in enumerate(boxes):\n",
    "                results.append({\n",
    "                    \"image_id\": image_id,\n",
    "                    \"class_id\": int(labels[idx]),\n",
    "                    \"rad_id\": \"wbf\",\n",
    "                    \"xmin\": box[0]* max_value,\n",
    "                    \"ymin\": box[1]* max_value,\n",
    "                    \"xmax\": box[2]* max_value,\n",
    "                    \"ymax\": box[3]* max_value,\n",
    "                    \"kfold\":kfold,\n",
    "                })\n",
    "        # if class is nothing then have it once (instead of 3 times in the same image)\n",
    "        if data.class_id.unique()[0] ==14:\n",
    "            for idx, box in enumerate([0]):\n",
    "                results.append({\n",
    "                    \"image_id\": image_id,\n",
    "                    \"class_id\": data.class_id[0],\n",
    "                    \"rad_id\": \"wbf\",\n",
    "                    \"xmin\": 0,\n",
    "                    \"ymin\": 0,\n",
    "                    \"xmax\": 1,\n",
    "                    \"ymax\": 1,\n",
    "                    \"kfold\":kfold,\n",
    "                })\n",
    "            \n",
    "    results = pd.DataFrame(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Pascal to coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pascal_to_coco(train_df):\n",
    "    # Good exlanation of coco, pascal etc \n",
    "    # https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n",
    "    train_df['coco_x'] = train_df['xmin'] + (train_df['xmax'] - train_df['xmin'] )/2\n",
    "    train_df['coco_y'] = train_df['ymin'] + (train_df['ymax'] - train_df['ymin'] )/2\n",
    "    train_df['coco_w'] = train_df['xmax'] - train_df['xmin'] \n",
    "    train_df['coco_h'] = train_df['ymax'] - train_df['ymin'] \n",
    "\n",
    "    train_df.loc[train_df['class_id'] == 14, 'coco_x'] = 1\n",
    "    train_df.loc[train_df['class_id'] == 14, 'coco_y'] = 1\n",
    "    train_df.loc[train_df['class_id'] == 14, 'coco_w'] = 0.5\n",
    "    train_df.loc[train_df['class_id'] == 14, 'coco_h'] = 0.5\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Main preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    train_df = read_images()\n",
    "    train_df = scale_images(train_df)\n",
    "    train_df = define_folds(train_df)\n",
    "    train_df = boxes_fusion(train_df)\n",
    "    train_df.set_index('image_id', inplace=True)\n",
    "    train_df = pascal_to_coco(train_df)\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating Image Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    # image augmentations for the training set\n",
    "    return A.Compose([A.ToGray(p=0.01),\n",
    "                      A.Cutout(num_holes=10, max_h_size=32, max_w_size=32, fill_value=0, p=0.5),\n",
    "                      ToTensorV2(p=1.0)],\n",
    "                      p=1.0,\n",
    "                      bbox_params=A.BboxParams(format='coco',min_area=0, min_visibility=0,label_fields=['labels'])\n",
    "                      )\n",
    "\n",
    "def get_valid_transforms():\n",
    "    # image augmentations for the validation set\n",
    "    return A.Compose([ToTensorV2(p=1.0)], \n",
    "                      p=1.0, \n",
    "                      bbox_params=A.BboxParams(format='coco',min_area=0, min_visibility=0,label_fields=['labels'])\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_TRAIN = '../input/vinbigdata-chest-xray-abnormalities-detection/train'\n",
    "DIR_TRAIN_PNG = '../input/vinbigdata-512-image-dataset/vinbigdata/train'\n",
    "\n",
    "import pydicom\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "\n",
    "class VinDataset(Dataset):\n",
    "    def __init__(self,image_ids,dataframe,transforms=None):\n",
    "        self.image_ids = image_ids\n",
    "        self.df = dataframe\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        image_id = self.image_ids[index]\n",
    "        records = self.df.loc[image_id]\n",
    "        labels = records['class_id']\n",
    "        \n",
    "        image = cv2.imread(f'{DIR_TRAIN_PNG}/{image_id}.png', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        \n",
    "        # DETR takes in data in coco format    \n",
    "        boxes = records[['coco_x', 'coco_y', 'coco_w', 'coco_h']].values\n",
    "     \n",
    "        # AS pointed out by PRVI It works better if the main class is labelled as zero\n",
    "        labels =  np.array(labels)\n",
    "    \n",
    "        if boxes.ndim == 1 : \n",
    "            boxes = np.expand_dims(boxes, axis=0)\n",
    "            labels = np.expand_dims(labels, axis=0)\n",
    "        \n",
    "        # AS pointed out by PRVI It works better if the main class is labelled as zero\n",
    "        labels =  np.array(labels)\n",
    "\n",
    "        if self.transforms:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': boxes,\n",
    "                'labels': labels\n",
    "            }\n",
    "\n",
    "        sample = self.transforms(**sample)\n",
    "        image = sample['image']\n",
    "        boxes = sample['bboxes']\n",
    "        labels = sample['labels']\n",
    "        \n",
    "        # Normalizing BBOXES\n",
    "        _,h,w = image.shape\n",
    "        boxes = A.augmentations.bbox_utils.normalize_bboxes(sample['bboxes'],rows=h,cols=w)\n",
    "\n",
    "        target = {}\n",
    "        target['boxes'] = torch.as_tensor(boxes,dtype=torch.float32)\n",
    "        target['labels'] = torch.as_tensor(labels,dtype=torch.long)\n",
    "        target['image_id'] = torch.tensor([index])\n",
    "\n",
    "        return image/255, target, image_id    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DETR model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class DETRModel(nn.Module):\n",
    "    def __init__(self,num_classes,num_queries):\n",
    "        super(DETRModel,self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_queries = num_queries\n",
    "        self.model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)\n",
    "        \n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n",
    "        self.in_features = self.model.class_embed.in_features\n",
    "        \n",
    "        self.model.class_embed = nn.Linear(in_features=self.in_features,out_features=self.num_classes+1)\n",
    "        self.model.num_queries = self.num_queries\n",
    "        \n",
    "    def forward(self,images):\n",
    "        return self.model(images)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
